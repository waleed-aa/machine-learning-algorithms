{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0693c388",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "Random Forests are an ensemble of decision trees typically trained using a (tweaked) bagging or pasting approach. The idea is that we train numerous weak learners and aggregate their predictive power. For classification problems, the predicted class that gets the most votes is chosen. For regression tasks, the mean of the individual decision trees is considered.\n",
    "\n",
    "The reason why an ensemble of weak learners produces a powerful model is due to the law of large numbers. According to the theory, if we repeat an experiment many times and average the results, the results obtained will be close to the theoretical expected value. Thus, if we create a weak learner that is only slightly better than random guessing (51%), a series of 1000 such learners will ultimately produce a much larger predictive power.\n",
    "\n",
    "\n",
    "# Training Process\n",
    "\n",
    "It’s important to note that random forests utilize a bagging-based resampling approach. This process involves taking multiple bootstapped (bagged) samples of the original dataset with replacement and each sample is used to train a seperate decision tree. \n",
    "\n",
    "A typical bagging based approach would train multiple trees and all trees would end up splitting at the most important feature. This results in a highly correlated prediction thereby producing high variance. Random forests address this problem by forcing each split based on a random sample of a predictor subset of $m$ predictors (column subsampling). The model then splits the trees based on the best feature within the subset. This increases tree diversity as some trees will contain predictors that would have been dismissed otherwise.\n",
    "\n",
    "To summarize, the primary difference between bagging and random forests is the criteria for choosing the predictor subset. In bagging we use all predictors $m = p$ vs while in random forests $m = √p$. This process is advantageous as it prevents all trees from splitting at the most important variable (thereby reducing variance). This decorrelation allows the model to reduce overfitting and produces greater tree diversity.\n",
    "\n",
    "\n",
    "# Random Forest Hyperparameters\n",
    "\n",
    "Random forests have almost all hyperparameters of decision trees and bagging classifiers. Tweaking these allows us to control tree growth and the ensemble process. At each tree split only a random subset of the features is considered. This randomness can be further exaggerated or constrained allowing us to control the bias-variance trade-off. Higher randomness typically increases bias but produces a more reliable model.\n",
    "\n",
    "- Node Size\n",
    "\n",
    "- Number of trees \n",
    "\n",
    "- Number of features sampled\n",
    "\n",
    "\n",
    "# Feature Importance\n",
    "\n",
    "Another interesting characteristic of random forest is that it measures the relative importance of each input feature. It computes feature importance using the largest relative mean decrease. For regression, we use the RSS decrease. A large value indicates a stronger predictor.\n",
    "\n",
    "\n",
    "# Random Forest Pros and Cons\n",
    "\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- Produce lower variance relative to decision trees and regular bagging as it decorrelates features.\n",
    "\n",
    "- Produces a stronger model at the cost of interpretability. Unlike a regular decision tree model, we cannot produce a decision tree diagram. When we bag numerous trees, it is not possible to visualize the statistical learning process through a decision tree diagram. \n",
    "\n",
    "- Allows us to determine feature importance. This could be used as a method conduct feature extraction.\n",
    "\n",
    "- Doesn't require data-pre-processing (feature scaling)\n",
    "\n",
    "- Generally robust against outliers.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- Trades better performance for lack of interpretability\n",
    "\n",
    "- Works well with non-linear datasets\n",
    "\n",
    "- Lots of Tuning parameters\n",
    "\n",
    "- Cannot be used in time series-based datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9627b",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a517d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5dfafba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LungCap</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Caesarean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.475</td>\n",
       "      <td>6</td>\n",
       "      <td>62.1</td>\n",
       "      <td>no</td>\n",
       "      <td>male</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.125</td>\n",
       "      <td>18</td>\n",
       "      <td>74.7</td>\n",
       "      <td>yes</td>\n",
       "      <td>female</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.550</td>\n",
       "      <td>16</td>\n",
       "      <td>69.7</td>\n",
       "      <td>no</td>\n",
       "      <td>female</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.125</td>\n",
       "      <td>14</td>\n",
       "      <td>71.0</td>\n",
       "      <td>no</td>\n",
       "      <td>male</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.800</td>\n",
       "      <td>5</td>\n",
       "      <td>56.9</td>\n",
       "      <td>no</td>\n",
       "      <td>male</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LungCap  Age  Height Smoke  Gender Caesarean\n",
       "0    6.475    6    62.1    no    male        no\n",
       "1   10.125   18    74.7   yes  female        no\n",
       "2    9.550   16    69.7    no  female       yes\n",
       "3   11.125   14    71.0    no    male        no\n",
       "4    4.800    5    56.9    no    male        no"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv('LungCapData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06657803",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64dc8c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((725, 8), (725,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictors and Target\n",
    "X = df.drop(columns = ['LungCap'])\n",
    "y = df['LungCap']\n",
    "\n",
    "# Instantiate one-hot encoder\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "# columns to be one hot encoded\n",
    "ct = make_column_transformer(\n",
    "\n",
    "    (ohe, ['Smoke', 'Gender', 'Caesarean']),\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "# predictors and target variable\n",
    "X = np.array(ct.fit_transform(X))\n",
    "y = np.array(y)\n",
    "\n",
    "# Checck input and target variable shape\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c80c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized feature Mean: 0.0\n",
      "Standardized feature SD : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training and Testing subsets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 911)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "print('Standardized feature Mean:',  X_train.mean().round())\n",
    "print('Standardized feature SD :',   X_train.std().round())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266da899",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2204834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Decision Tree Classifier on default parameters\n",
    "rf = RandomForestRegressor(random_state = 0)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4d321",
   "metadata": {},
   "source": [
    "# 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5823c54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error : 1.2464510183973299\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Mean squared error\n",
    "print('Mean Squared Error :', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e1263",
   "metadata": {},
   "source": [
    "# 5. K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa870d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75984122 0.82651557 0.84983209 0.73433377 0.82721232 0.8618788\n",
      " 0.79488641 0.83534297 0.81926248 0.69429939]\n",
      "R2: 80.034 %\n",
      "R2 Standard Deviation: 5.148 %\n"
     ]
    }
   ],
   "source": [
    "# 10 fold cross validation\n",
    "R2 = cross_val_score(estimator = RandomForestRegressor(),\n",
    "                             X = X,\n",
    "                             y = y,\n",
    "                             cv = 10)\n",
    "\n",
    "# Cross validation accuracy and standard deviation\n",
    "print(R2)\n",
    "print(\"R2: {:.3f} %\".format(R2.mean()*100))\n",
    "print(\"R2 Standard Deviation: {:.3f} %\".format(R2.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054560c",
   "metadata": {},
   "source": [
    "# 6. Hyperparametric Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60e60fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best R2: 82.01 %\n",
      "Best Parameters: {'bootstrap': True, 'max_depth': 6, 'max_features': 'auto', 'min_samples_leaf': 5, 'min_samples_split': 4, 'n_estimators': 350}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search CV\n",
    "param_grid = [{'bootstrap': [True],\n",
    "     'max_depth': [6, 10],\n",
    "     'max_features': ['auto'],\n",
    "     'min_samples_leaf': [3, 5],\n",
    "     'min_samples_split': [4, 6],\n",
    "     'n_estimators': [100, 350]}]\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Configure GridSearchCV\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5,\n",
    "                                  scoring=\"r2\",\n",
    "                                  n_jobs=-1)\n",
    "# Initiate Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract Tuned Parameters and Predictive Accuracy\n",
    "tuned_params = grid_search.best_params_\n",
    "tuned_score = grid_search.best_score_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Print Results\n",
    "print(\"Best R2: {:.2f} %\".format(grid_search.best_score_*100))\n",
    "print(\"Best Parameters:\", tuned_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295a3e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 81.84 %\n",
      "Best Parameters: {'n_estimators': 400, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 6, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "# Randomized Search\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "param_space = {\"bootstrap\": [True],\n",
    "        \"max_depth\": [6, 8, 10, 12, 14],\n",
    "        \"max_features\": ['auto'],\n",
    "        \"min_samples_leaf\": [2, 3, 4],\n",
    "        \"min_samples_split\": [2, 3, 4, 5],\n",
    "        \"n_estimators\": [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "}\n",
    "\n",
    "# Configure Randomized Search\n",
    "random_search = RandomizedSearchCV(rf, param_space,\n",
    "                                        scoring=\"r2\", cv=5,\n",
    "                                        n_jobs=-1, random_state=911)\n",
    "# Initiate Search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract Tuned Parameters and Predictive Accuracy\n",
    "tuned_params = random_search.best_params_\n",
    "tuned_score = random_search.best_score_\n",
    "best_estimator = random_search.best_estimator_\n",
    "\n",
    "# Print accuracy and best parameters\n",
    "print(\"Best Accuracy: {:.2f} %\".format(random_search.best_score_*100))\n",
    "print(\"Best Parameters:\", tuned_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
